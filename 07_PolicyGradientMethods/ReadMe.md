
## Policy Gradient Methods 

### Contents (From Chapter 13 in Sutton and Barto Book)
- Policy Approximation and its Advantages
- The Policy Gradient Theorem
- REINFORCE: Monte Carlo Policy Gradient
- REINFORCE with Baseline
- Actor-Critic Methods
- Policy Gradient for Continuing Problems
- Policy Parameterization for Continuous Actions

### Summary Points
- Idea: Instead of parameterizing the value function and performing greedy policy improvement we parameterize the policy and do gradient descent into a direction that improves it.

### Solutions to Exercises



