# Contents: Multi-Armed Bandits (MAB)
This will focus on Multi Armed Bandit Problems.

- [Topics in Sutton and Barto Book](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#topics-in-sutton-and-barto-book)
- [Understanding Points](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#understanding-points)
- [Exercises and Problem Solving](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#exercises-and-problem-solving)


## Topics in Sutton and Barto Book
- A k-armed Bandit Problem
- Action-Value Methods
- The 10-armed TestBed
- Incremental Implementation
- Tracking a NonStationary Problem
- Optimistic Initial Values
- UCB Action Selection
- Gradient Bandit Algorithms
- Associative Search (Contextual Bandits)

[Back to Top](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#contents-multi-armed-bandits-mab)

## Understanding Points
- Multi-armed Bandits is a basic RL setting onvolving one state but multiple actions
- Action-Value Methods
  - Estimate the values of actions
  - Use the estimates obtained for action selection decisions
- MAB Compare : A parameter study of various bandit algorithms used (Epsilon-Greedy, UCB, Gradient Bandit, Greedy with Optimistic Initialization etc.). Each point is the average reward obtained over 1000 steps with a particular algorithm at a particular setting of its parameter.
![MAB Compare](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/MAB_Compare.png)

[Back to Top](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#contents-multi-armed-bandits-mab)

## Exercises and Problem Solving


[Back to Top](https://github.com/kkm24132/ReinforcementLearning/blob/main/02_MAB/ReadMe.md#contents-multi-armed-bandits-mab)
