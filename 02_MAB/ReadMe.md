# Contents: Multi-Armed Bandits (MAB)
This will focus on Multi Armed Bandit Problems.

- Topics in Sutton and Barto Book
- Understanding Points
- Exercises and Problem Solving

## Topics in Sutton and Barto Book
- A k-armed Bandit Problem
- Action-Value Methods
- The 10-armed TestBed
- Incremental Implementation
- Tracking a NonStationary Problem
- Optimistic Initial Values
- UCB Action Selection
- Gradient Bandit Algorithms
- Associative Search (Contextual Bandits)

## Understanding Points
- Multi-armed Bandits is a basic RL setting onvolving one state but multiple actions
- Action-Value Methods
  - Estimate the values of actions
  - Use the estimates obtained for action selection decisions

## Exercises and Problem Solving
